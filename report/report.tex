% !TEX program = xelatex
\documentclass{article}
\usepackage{graphicx,geometry,hyperref,float}
% Justify Paragrapher
\newcommand{\justified}
{
	\tolerance=1
	\emergencystretch=\maxdimen
	\hyphenpenalty=10000
	\hbadness=10000
}


\graphicspath{ {./images/} }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{
	\vspace{4cm}
	\textbf{Data And Information Quality Project Report}\\
}
\author{}
\date{}

\begin{document}
\maketitle
\vfill
\justified
\begin{itemize}
	\item\textbf{PROJECT ID}: 21
	\item\textbf{PROJECT NUMBER}: 1
	\item\textbf{ASSIGNED DATASET}: USERS
	\item\textbf{STUDENT}: PASQUALE CASTIGLIONE 10657816
	\item\textbf{ASSIGNED TASK}: CLUSTERING
\end{itemize}

\newpage

\tableofcontents

\newpage



\section{SETUP CHOICES}
\subsection{Clustering}
Clustering was performed used two different methods: K-Modes and K-Means.
\begin{itemize}
\item\textbf{K-Modes}
Because of the categorical nature of the data, \emph{K-Modes} was used. In order to select the number of clusters, \emph{elbow} method analysis was performed.
\item\textbf{K-Means}
The second clustering techniques used was K-Means using Jaccard as distance measure.
\end{itemize}
\subsection{Chosen ML performance evaluation metrics}
\subsection{Imputation}
\subsubsection{Simple Imputation}
Simple imputation was performed by propagating valid values to the cells with missing values using \emph{fillna} method from \emph{Pandas}. Firstly values were propagated forward then, to avoid null values in the first row, values were propagated backward also. This method showed very good result with the original data, but applying it to a shuffle version of the dataset showed worse results.  
\subsubsection{Advanced Imputation}
\emph{K-Nearest Neighbors} was used as the advanced technique used to impute missing value. In order to apply this method, data were firstly encoded as one-hot numeric arrays, then \emph{KNNImputer} from \emph{sklearn} was used to fit and transform the dirty dataset.
This method turned out to be robust to shuffling.
\section{PIPELINE IMPLEMENTATION}
\subsection{Description of the steps you performed}
\begin{itemize}
	\item\textbf{Data Exploration:} In this phase histograms of the datasets and heatmaps of the datasets with missing values were plotted.
	\item\textbf{Imputation:} In this phase imputation was performed.
		In particular for the advanced imputation, features were encoded as one hot vector using the \emph{get\_dummies()} method from \emph{pandas}
\end{itemize}
\newpage
\section{RESULTS}

\subsection{Imputation}
To assess the quality of the imputation, accuracy was computed as the number of rows that were imputed as the original over all the rows.

\subsubsection{Simple Imputation Accuracy}

\begin{table}[h]
\begin{minipage}{.45\textwidth}\centering
\begin{tabular}{ccccc}
              & \textbf{CT} & \textbf{CU} & \textbf{LT} & \textbf{TC} \\
\textbf{50\%} & 0.92        & 0.91        & 0.9         & 1.00        \\
\textbf{60\%} & 0.92        & 0.93        & 0.93        & 1.00        \\
\textbf{70\%} & 0.94        & 0.97        & 0.93        & 0.99        \\
\textbf{80\%} & 0.96        & 0.95        & 0.97        & 1.00        \\
\textbf{90\%} & 0.97        & 0.99        & 0.99        & 1.00       
\end{tabular}
\caption{Simple Imputation}
\end{minipage}
\hfill
\begin{minipage}{.45\textwidth}\centering
\begin{tabular}{ccccc}
              & \textbf{CT} & \textbf{CU} & \textbf{LT} & \textbf{TC} \\
\textbf{50\%} & 0.69        & 0.81        & 0.67        & 0.61        \\
\textbf{60\%} & 0.71        & 0.83        & 0.76        & 0.69        \\
\textbf{70\%} & 0.83        & 0.90        & 0.81        & 0.78        \\
\textbf{80\%} & 0.89        & 0.93        & 0.87        & 0.83        \\
\textbf{90\%} & 0.91        & 0.98        & 0.94        & 0.91       
\end{tabular}
\caption{Simple Imputation Shuffled}
\end{minipage}
\end{table}

\subsubsection{Advanced Imputation Accuracy}
\begin{table}[h]
\begin{minipage}{.45\textwidth}\centering
\begin{tabular}{ccccc}
              & \textbf{CT} & \textbf{CU} & \textbf{LT} & \textbf{TC} \\
\textbf{50\%} & 0.76        & 0.90        & 0.79        & 0.80        \\
\textbf{60\%} & 0.84        & 0.92        & 0.85        & 0.87        \\
\textbf{70\%} & 0.88        & 0.96        & 0.92        & 0.91        \\
\textbf{80\%} & 0.94        & 0.97        & 0.95        & 0.94        \\
\textbf{90\%} & 0.97        & 1.00        & 0.99        & 0.98       
\end{tabular}
\caption{KNN Imputation}
\end{minipage}
\hfill
\begin{minipage}{.45\textwidth}\centering
\begin{tabular}{ccccc}
              & \textbf{CT} & \textbf{CU} & \textbf{LT} & \textbf{TC} \\
\textbf{50\%} & 0.79        & 0.91        & 0.78        & 0.78        \\
\textbf{60\%} & 0.83        & 0.91        & 0.86        & 0.83        \\
\textbf{70\%} & 0.88        & 0.94        & 0.91        & 0.91        \\
\textbf{80\%} & 0.96        & 0.96        & 0.94        & 0.95        \\
\textbf{90\%} & 0.98        & 0.99        & 0.99        & 1.00       
\end{tabular}
\caption{KNN Imputation Shuffled}
\end{minipage}
\end{table}

\subsection{ML performance comparison between the imputation/outlier detection techniques you have implemented}

\end{document}
